{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "1O6zaCh0Zxlh",
        "outputId": "c0e04d0e-af3b-47dc-c945-c0f453fbd0b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 186.12 MiB is free. Process 9082 has 14.56 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 42.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a46f71097a2c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-a46f71097a2c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/fasterrcnn_resnet50_davis.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a46f71097a2c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;31m# Enhanced training function with F1, accuracy, and loss tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 186.12 MiB is free. Process 9082 has 14.56 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 42.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision scipy numpy pillow matplotlib opencv-python torchaudio\n",
        "!pip install davis2017-evaluation\n",
        "\n",
        "# # Download and extract DAVIS 2017 dataset\n",
        "!wget -O DAVIS-2017-trainval-480p.zip https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip\n",
        "!unzip -q DAVIS-2017-trainval-480p.zip -d /content/DAVIS\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import functional as F\n",
        "from IPython.display import Image as IPImage, display\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "\n",
        "# Define the DAVIS Dataset class\n",
        "class DAVISDataset(Dataset):\n",
        "    def __init__(self, davis_root, subset='val', transform=None):\n",
        "        self.davis_root = davis_root\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "        self.img_dir = os.path.join(davis_root, 'JPEGImages', '480p')\n",
        "        self.anno_dir = os.path.join(davis_root, 'Annotations', '480p')\n",
        "        with open(os.path.join(davis_root, 'ImageSets', '2017', f'{subset}.txt'), 'r') as f:\n",
        "            self.sequences = f.read().splitlines()\n",
        "        self.class_map = {\n",
        "            'bear': 1, 'bmx-bumps': 2, 'boat': 3, 'boxing-fisheye': 4, 'breakdance-flare': 5,\n",
        "            'bus': 6, 'car-turn': 7, 'cat-girl': 8, 'classic-car': 9, 'color-run': 10,\n",
        "            'crossing': 11, 'dance-jump': 12, 'dancing': 13, 'disc-jockey': 14, 'dog-agility': 15,\n",
        "            'dog-gooses': 16, 'dogs-scale': 17, 'drift-turn': 18, 'drone': 19, 'elephant': 20,\n",
        "            'flamingo': 21, 'hike': 22, 'hockey': 23, 'horsejump-low': 24, 'kid-football': 25,\n",
        "            'kite-walk': 26, 'koala': 27, 'lady-running': 28, 'lindy-hop': 29, 'longboard': 30,\n",
        "            'lucia': 31, 'mallard-fly': 32, 'mallard-water': 33, 'miami-surf': 34, 'motocross-bumps': 35,\n",
        "            'motorbike': 36, 'night-race': 37, 'paragliding': 38, 'planes-water': 39, 'rallye': 40,\n",
        "            'rhino': 41, 'rollerblade': 42, 'schoolgirls': 43, 'scooter-board': 44, 'scooter-gray': 45,\n",
        "            'sheep': 46, 'skate-park': 47, 'snowboard': 48, 'soccerball': 49, 'stroller': 50,\n",
        "            'stunt': 51, 'surf': 52, 'swing': 53, 'tennis': 54, 'tractor-sand': 55,\n",
        "            'train': 56, 'tuk-tuk': 57, 'upside-down': 58, 'varanus-cage': 59, 'walking': 60,\n",
        "            'bike-packing': 61, 'blackswan': 62, 'bmx-trees': 63, 'breakdance': 64, 'camel': 65,\n",
        "            'car-roundabout': 66, 'car-shadow': 67, 'cows': 68, 'dance-twirl': 69, 'dog': 70,\n",
        "            'dogs-jump': 71, 'drift-chicane': 72, 'drift-straight': 73, 'goat': 74, 'gold-fish': 75,\n",
        "            'horsejump-high': 76, 'india': 77, 'judo': 78, 'kite-surf': 79, 'lab-coat': 80,\n",
        "            'libby': 81, 'loading': 82, 'mbike-trick': 83, 'motocross-jump': 84, 'paragliding-launch': 85,\n",
        "            'parkour': 86, 'pigs': 87, 'scooter-black': 88, 'shooting': 89, 'soapbox': 90\n",
        "        }\n",
        "        self.class_names = {v: k for k, v in self.class_map.items()}\n",
        "        self.samples = []\n",
        "        for seq in self.sequences:\n",
        "            img_seq_dir = os.path.join(self.img_dir, seq)\n",
        "            anno_seq_dir = os.path.join(self.anno_dir, seq)\n",
        "            img_files = sorted(os.listdir(img_seq_dir))\n",
        "            for img_file in img_files:\n",
        "                if img_file.endswith('.jpg'):\n",
        "                    frame_num = img_file.split('.')[0]\n",
        "                    anno_file = f\"{frame_num}.png\"\n",
        "                    if os.path.exists(os.path.join(anno_seq_dir, anno_file)):\n",
        "                        self.samples.append((\n",
        "                            os.path.join(img_seq_dir, img_file),\n",
        "                            os.path.join(anno_seq_dir, anno_file),\n",
        "                            seq\n",
        "                        ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, seq_name = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        image_np = np.array(image)\n",
        "        mask_np = np.array(mask)\n",
        "        obj_ids = np.unique(mask_np)[1:] if np.unique(mask_np).size > 1 else []\n",
        "        if len(obj_ids) == 0:\n",
        "            masks = []\n",
        "        else:\n",
        "            masks = mask_np == obj_ids[:, None, None]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        class_label = self.class_map.get(seq_name, 1)\n",
        "        for i, mask in enumerate(masks):\n",
        "            pos = np.where(mask)\n",
        "            if len(pos[0]) > 0:\n",
        "                xmin = np.min(pos[1])\n",
        "                xmax = np.max(pos[1])\n",
        "                ymin = np.min(pos[0])\n",
        "                ymax = np.max(pos[0])\n",
        "                if xmax > xmin and ymax > ymin:\n",
        "                    boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    labels.append(class_label)\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros(0, dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': image_id,\n",
        "            'area': area,\n",
        "            'iscrowd': iscrowd\n",
        "        }\n",
        "        if self.transform:\n",
        "            image, target = self.transform(image, target)\n",
        "        return image, target\n",
        "\n",
        "# Define transformation classes\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "class Normalize:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, target\n",
        "\n",
        "class RandomHorizontalFlip:\n",
        "    def __init__(self, prob=0.5):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = F.hflip(image)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "            target[\"boxes\"] = bbox\n",
        "        return image, target\n",
        "\n",
        "# Define the Faster R-CNN model\n",
        "def create_fasterrcnn_model(num_classes, pretrained_backbone=True):\n",
        "    import torchvision\n",
        "    backbone = torchvision.models.resnet50(weights='DEFAULT' if pretrained_backbone else None)\n",
        "    backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
        "    backbone.out_channels = 2048\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "    )\n",
        "    roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0'],\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "    model = FasterRCNN(\n",
        "        backbone=backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler,\n",
        "        min_size=800,\n",
        "        max_size=1333\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    \"\"\" Compute IoU between two boxes: box1 and box2 are tensors (x1, y1, x2, y2) \"\"\"\n",
        "    xA = torch.max(box1[0], box2[0])\n",
        "    yA = torch.max(box1[1], box2[1])\n",
        "    xB = torch.min(box1[2], box2[2])\n",
        "    yB = torch.min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, xB - xA) * max(0, yB - yA)\n",
        "    box1_area = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
        "    box2_area = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
        "    union = box1_area + box2_area - inter_area\n",
        "    return inter_area / union if union != 0 else 0\n",
        "\n",
        "# Enhanced training function with F1, accuracy, and loss tracking\n",
        "def train_model(model, data_loader, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    loss_history, acc_history, f1_history = [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_running_loss = 0.0\n",
        "        all_preds, all_targets = [], []\n",
        "\n",
        "        for images, targets in data_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_running_loss += total_loss.item()\n",
        "\n",
        "            # Prediction for metrics\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = model(images)\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                if len(output['boxes']) == 0 or len(target['boxes']) == 0:\n",
        "                    warnings.warn(\"Empty prediction or target, skipping this sample.\")\n",
        "                    continue\n",
        "\n",
        "                pred_boxes = output['boxes'].cpu()\n",
        "                pred_labels = output['labels'].cpu()\n",
        "                tgt_boxes = target['boxes'].cpu()\n",
        "                tgt_labels = target['labels'].cpu()\n",
        "\n",
        "                # Simple match logic using IoU > 0.5\n",
        "                matched_preds, matched_targets = [], []\n",
        "                for i, tgt_box in enumerate(tgt_boxes):\n",
        "                    best_iou, best_idx = 0, -1\n",
        "                    for j, pred_box in enumerate(pred_boxes):\n",
        "                        iou = compute_iou(tgt_box, pred_box)\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_idx = j\n",
        "                    if best_iou > 0.5:\n",
        "                        matched_preds.append(pred_labels[best_idx].item())\n",
        "                        matched_targets.append(tgt_labels[i].item())\n",
        "\n",
        "                if matched_preds:\n",
        "                    all_preds.extend(matched_preds)\n",
        "                    all_targets.extend(matched_targets)\n",
        "\n",
        "        # Compute metrics\n",
        "        epoch_loss = epoch_running_loss / len(data_loader)\n",
        "        epoch_acc = accuracy_score(all_targets, all_preds) if all_preds else 0.0\n",
        "        epoch_f1 = f1_score(all_targets, all_preds, average='weighted') if all_preds else 0.0\n",
        "\n",
        "        loss_history.append(epoch_loss)\n",
        "        acc_history.append(epoch_acc)\n",
        "        f1_history.append(epoch_f1)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, F1: {epoch_f1:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Add this validation function after the train_model function\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    all_preds, all_targets = [], []\n",
        "    val_loss = 0.0 # Initialize val_loss\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Forward pass to calculate loss\n",
        "            loss_dict = model(images, targets) # Calculate loss\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            val_loss += losses.item() # Accumulate loss\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                if len(output['boxes']) == 0 or len(target['boxes']) == 0:\n",
        "                    warnings.warn(\"Empty prediction or target in validation.\")\n",
        "                    continue\n",
        "\n",
        "                pred_boxes = output['boxes'].cpu()\n",
        "                pred_labels = output['labels'].cpu()\n",
        "                tgt_boxes = target['boxes'].cpu()\n",
        "                tgt_labels = target['labels'].cpu()\n",
        "\n",
        "                matched_preds, matched_targets = [], []\n",
        "                for i, tgt_box in enumerate(tgt_boxes):\n",
        "                    best_iou, best_idx = 0, -1\n",
        "                    for j, pred_box in enumerate(pred_boxes):\n",
        "                        iou = compute_iou(tgt_box, pred_box)\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_idx = j\n",
        "                    if best_iou > 0.5:\n",
        "                        matched_preds.append(pred_labels[best_idx].item())\n",
        "                        matched_targets.append(tgt_labels[i].item())\n",
        "\n",
        "                    if matched_preds:\n",
        "                        all_preds.extend(matched_preds)\n",
        "                        all_targets.extend(matched_targets)\n",
        "    # Calculate average loss\n",
        "    val_loss /= len(data_loader)\n",
        "\n",
        "    val_acc = accuracy_score(all_targets, all_preds) if all_preds else 0.0\n",
        "    val_f1 = f1_score(all_targets, all_preds, average='weighted') if all_preds else 0.0\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}\") # Print validation loss\n",
        "\n",
        "    return val_loss, val_acc, val_f1\n",
        "\n",
        "\n",
        "def plot_validation_metrics(val_losses, val_accuracies, val_f1_scores):\n",
        "    epochs = list(range(1, len(val_losses)+1))\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, val_losses, 'r-o')\n",
        "    plt.title(\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, val_accuracies, 'g-o')\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, val_f1_scores, 'b-o')\n",
        "    plt.title(\"Validation F1 Score\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_curves(loss_history, acc_history, f1_history):\n",
        "    epochs = range(1, len(loss_history)+1)\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(epochs, loss_history, 'r-', label='Loss')\n",
        "    plt.plot(epochs, acc_history, 'b--', label='Accuracy')\n",
        "    plt.plot(epochs, f1_history, 'g-.', label='F1 Score')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Metric\")\n",
        "    plt.title(\"Training Metrics Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Function to extract frames\n",
        "def extract_frames(video_path, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return None\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_paths = []\n",
        "    frame_count = 0\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame_count += 1\n",
        "            frame_path = os.path.join(output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            frame_paths.append(frame_path)\n",
        "            print(f\"Extracting frame {frame_count}/{total_frames}\", end='\\r')\n",
        "    except Exception as e:\n",
        "        print(f\"Error during frame extraction: {str(e)}\")\n",
        "        cap.release()\n",
        "        return None\n",
        "    cap.release()\n",
        "    print(f\"\\nExtracted {frame_count} frames to {output_dir}\")\n",
        "    return frame_paths, width, height, fps\n",
        "\n",
        "# Function to visualize detections\n",
        "def visualize_detections(image, prediction, class_names, threshold=0.3):\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    image_np = (image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
        "    image_np = image_np.astype(np.uint8).copy()\n",
        "    for box, label, score in zip(prediction['boxes'].cpu().numpy(),\n",
        "                                prediction['labels'].cpu().numpy(),\n",
        "                                prediction['scores'].cpu().numpy()):\n",
        "        if score >= threshold:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            class_name = class_names.get(label, 'Unknown')\n",
        "            cv2.rectangle(image_np, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            cv2.putText(image_np, f'{class_name}: {score:.2f}', (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "    return image_np\n",
        "\n",
        "# Function to process video frames\n",
        "def process_video_frames(model, video_path, class_names, device, frames_dir='/content/frames', output_frames_dir='/content/output_frames', threshold=0.3):\n",
        "    if os.path.exists(frames_dir):\n",
        "        shutil.rmtree(frames_dir)\n",
        "    if os.path.exists(output_frames_dir):\n",
        "        shutil.rmtree(output_frames_dir)\n",
        "    os.makedirs(frames_dir)\n",
        "    os.makedirs(output_frames_dir)\n",
        "    frame_paths, width, height, fps = extract_frames(video_path, frames_dir)\n",
        "    if frame_paths is None:\n",
        "        print(\"Error: Frame extraction failed.\")\n",
        "        return None\n",
        "    total_frames = len(frame_paths)\n",
        "    if total_frames == 0:\n",
        "        print(\"Error: No frames extracted from video.\")\n",
        "        return None\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    model.eval()\n",
        "    sample_frames = []\n",
        "    frame_count = 0\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for frame_path in frame_paths:\n",
        "                frame_count += 1\n",
        "                print(f\"Processing frame {frame_count}/{total_frames}\", end='\\r')\n",
        "                frame_rgb = cv2.cvtColor(cv2.imread(frame_path), cv2.COLOR_BGR2RGB)\n",
        "                image = Image.fromarray(frame_rgb)\n",
        "                image_tensor, _ = transform(image, {})\n",
        "                image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "                outputs = model(image_tensor)[0]\n",
        "                annotated_frame = visualize_detections(image_tensor[0], outputs, class_names, threshold)\n",
        "                output_frame_path = os.path.join(output_frames_dir, f\"annotated_frame_{frame_count:06d}.jpg\")\n",
        "                cv2.imwrite(output_frame_path, cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
        "                if frame_count % (total_frames // 5 + 1) == 0 and len(sample_frames) < 5:\n",
        "                    sample_frames.append(annotated_frame)\n",
        "                del image_tensor, outputs\n",
        "                torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during frame processing: {str(e)}\")\n",
        "        return None\n",
        "    print(f\"\\nProcessed {frame_count} frames. Annotated frames saved to {output_frames_dir}\")\n",
        "    for i, frame in enumerate(sample_frames):\n",
        "        sample_path = f'/content/sample_frame_{i}.png'\n",
        "        cv2.imwrite(sample_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "        display(IPImage(sample_path))\n",
        "    shutil.rmtree(frames_dir)\n",
        "    print(\"Cleaned up temporary frames directory\")\n",
        "    return output_frames_dir\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    davis_root = '/content/DAVIS/DAVIS'\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        RandomHorizontalFlip(prob=0.5)\n",
        "    ])\n",
        "\n",
        "    # Optional: Train the model (uncomment to train)\n",
        "    train_dataset = DAVISDataset(davis_root, subset='train', transform=transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "    num_classes = 91  # Adjust num_classes\n",
        "    model = create_fasterrcnn_model(num_classes=num_classes)\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    model = train_model(model, train_loader, optimizer, 1, device)\n",
        "    torch.save(model.state_dict(), '/content/fasterrcnn_resnet50_davis.pth')\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    # Load dataset for class names\n",
        "    dataset = DAVISDataset(davis_root, subset='val', transform=transform)\n",
        "    class_names = dataset.class_names\n",
        "\n",
        "    from dataset import get_davis_dataloader  # Custom dataloader\n",
        "    from model import get_model  # Your Faster R-CNN model setup\n",
        "\n",
        "    # Load model and data\n",
        "    model = get_model(num_classes=91)  # Adjust num_classes\n",
        "    model.to(device)\n",
        "\n",
        "    _, val_loader = get_davis_dataloader(batch_size=2)  # Custom function\n",
        "\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    num_epochs = 3\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        val_loss, val_acc, val_f1 = validate_model(model, val_loader, device)\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        val_f1_scores.append(val_f1)\n",
        "\n",
        "    # Plot all metrics after validation\n",
        "    plot_validation_metrics(val_losses, val_accuracies, val_f1_scores)\n",
        "\n",
        "    # Load the model\n",
        "    num_classes = 91\n",
        "    model = create_fasterrcnn_model(num_classes=num_classes)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('/content/fasterrcnn_resnet50_davis.pth', map_location=device))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Model weights file 'fasterrcnn_resnet50_davis.pth' not found.\")\n",
        "        print(\"Please upload the trained model or uncomment the training section to train the model.\")\n",
        "        print(\"To proceed without training, upload 'fasterrcnn_resnet50_davis.pth' now:\")\n",
        "        uploaded = files.upload()\n",
        "        if 'fasterrcnn_resnet50_davis.pth' in uploaded:\n",
        "            model.load_state_dict(torch.load('/content/fasterrcnn_resnet50_davis.pth', map_location=device))\n",
        "        else:\n",
        "            print(\"Error: Model weights not uploaded. Cannot proceed.\")\n",
        "            return\n",
        "    model.to(device)\n",
        "\n",
        "    # Upload video\n",
        "    print(\"Please upload your video file (e.g., MP4 format):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"Error: No video file uploaded.\")\n",
        "        return\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # Process video frames\n",
        "    output_frames_dir = process_video_frames(\n",
        "        model, video_path, class_names, device,\n",
        "        frames_dir='/content/frames',\n",
        "        output_frames_dir='/content/output_frames',\n",
        "        threshold=0.3\n",
        "    )\n",
        "\n",
        "    if output_frames_dir:\n",
        "        print(f\"Processed frames saved to {output_frames_dir}\")\n",
        "        print(\"To download all frames, run the following cell to zip the output_frames directory:\")\n",
        "        print(\"Then download the zip file from the Colab file explorer.\")\n",
        "        # Zip the output frames for easy download\n",
        "        !zip -r /content/output_frames.zip /content/output_frames\n",
        "    else:\n",
        "        print(\"Frame processing failed. Please check error messages.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the updated version of the original code with validation funtions.\n",
        "# Install required packages\n",
        "!pip install torch torchvision scipy numpy pillow matplotlib opencv-python\n",
        "!pip install davis2017-evaluation\n",
        "!pip install --upgrade torch torchvision torchaudio\n",
        "\n",
        "# # Download and extract DAVIS 2017 dataset\n",
        "!wget -O DAVIS-2017-trainval-480p.zip https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip\n",
        "!unzip -q DAVIS-2017-trainval-480p.zip -d /content/DAVIS\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import functional as F\n",
        "from IPython.display import Image as IPImage, display\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Define the DAVIS Dataset class\n",
        "class DAVISDataset(Dataset):\n",
        "    def __init__(self, davis_root, subset='val', transform=None):\n",
        "        self.davis_root = davis_root\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "        self.img_dir = os.path.join(davis_root, 'JPEGImages', '480p')\n",
        "        self.anno_dir = os.path.join(davis_root, 'Annotations', '480p')\n",
        "        with open(os.path.join(davis_root, 'ImageSets', '2017', f'{subset}.txt'), 'r') as f:\n",
        "            self.sequences = f.read().splitlines()\n",
        "        self.class_map = {\n",
        "            'bear': 1, 'bmx-bumps': 2, 'boat': 3, 'boxing-fisheye': 4, 'breakdance-flare': 5,\n",
        "            'bus': 6, 'car-turn': 7, 'cat-girl': 8, 'classic-car': 9, 'color-run': 10,\n",
        "            'crossing': 11, 'dance-jump': 12, 'dancing': 13, 'disc-jockey': 14, 'dog-agility': 15,\n",
        "            'dog-gooses': 16, 'dogs-scale': 17, 'drift-turn': 18, 'drone': 19, 'elephant': 20,\n",
        "            'flamingo': 21, 'hike': 22, 'hockey': 23, 'horsejump-low': 24, 'kid-football': 25,\n",
        "            'kite-walk': 26, 'koala': 27, 'lady-running': 28, 'lindy-hop': 29, 'longboard': 30,\n",
        "            'lucia': 31, 'mallard-fly': 32, 'mallard-water': 33, 'miami-surf': 34, 'motocross-bumps': 35,\n",
        "            'motorbike': 36, 'night-race': 37, 'paragliding': 38, 'planes-water': 39, 'rallye': 40,\n",
        "            'rhino': 41, 'rollerblade': 42, 'schoolgirls': 43, 'scooter-board': 44, 'scooter-gray': 45,\n",
        "            'sheep': 46, 'skate-park': 47, 'snowboard': 48, 'soccerball': 49, 'stroller': 50,\n",
        "            'stunt': 51, 'surf': 52, 'swing': 53, 'tennis': 54, 'tractor-sand': 55,\n",
        "            'train': 56, 'tuk-tuk': 57, 'upside-down': 58, 'varanus-cage': 59, 'walking': 60,\n",
        "            'bike-packing': 61, 'blackswan': 62, 'bmx-trees': 63, 'breakdance': 64, 'camel': 65,\n",
        "            'car-roundabout': 66, 'car-shadow': 67, 'cows': 68, 'dance-twirl': 69, 'dog': 70,\n",
        "            'dogs-jump': 71, 'drift-chicane': 72, 'drift-straight': 73, 'goat': 74, 'gold-fish': 75,\n",
        "            'horsejump-high': 76, 'india': 77, 'judo': 78, 'kite-surf': 79, 'lab-coat': 80,\n",
        "            'libby': 81, 'loading': 82, 'mbike-trick': 83, 'motocross-jump': 84, 'paragliding-launch': 85,\n",
        "            'parkour': 86, 'pigs': 87, 'scooter-black': 88, 'shooting': 89, 'soapbox': 90\n",
        "        }\n",
        "        self.class_names = {v: k for k, v in self.class_map.items()}\n",
        "        self.samples = []\n",
        "        for seq in self.sequences:\n",
        "            img_seq_dir = os.path.join(self.img_dir, seq)\n",
        "            anno_seq_dir = os.path.join(self.anno_dir, seq)\n",
        "            img_files = sorted(os.listdir(img_seq_dir))\n",
        "            for img_file in img_files:\n",
        "                if img_file.endswith('.jpg'):\n",
        "                    frame_num = img_file.split('.')[0]\n",
        "                    anno_file = f\"{frame_num}.png\"\n",
        "                    if os.path.exists(os.path.join(anno_seq_dir, anno_file)):\n",
        "                        self.samples.append((\n",
        "                            os.path.join(img_seq_dir, img_file),\n",
        "                            os.path.join(anno_seq_dir, anno_file),\n",
        "                            seq\n",
        "                        ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, seq_name = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        image_np = np.array(image)\n",
        "        mask_np = np.array(mask)\n",
        "        obj_ids = np.unique(mask_np)[1:] if np.unique(mask_np).size > 1 else []\n",
        "        if len(obj_ids) == 0:\n",
        "            masks = []\n",
        "        else:\n",
        "            masks = mask_np == obj_ids[:, None, None]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        class_label = self.class_map.get(seq_name, 1)\n",
        "        for i, mask in enumerate(masks):\n",
        "            pos = np.where(mask)\n",
        "            if len(pos[0]) > 0:\n",
        "                xmin = np.min(pos[1])\n",
        "                xmax = np.max(pos[1])\n",
        "                ymin = np.min(pos[0])\n",
        "                ymax = np.max(pos[0])\n",
        "                if xmax > xmin and ymax > ymin:\n",
        "                    boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    labels.append(class_label)\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros(0, dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': image_id,\n",
        "            'area': area,\n",
        "            'iscrowd': iscrowd\n",
        "        }\n",
        "        if self.transform:\n",
        "            image, target = self.transform(image, target)\n",
        "        return image, target\n",
        "\n",
        "# Define transformation classes\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "class Normalize:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, target\n",
        "\n",
        "class RandomHorizontalFlip:\n",
        "    def __init__(self, prob=0.5):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = F.hflip(image)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "            target[\"boxes\"] = bbox\n",
        "        return image, target\n",
        "\n",
        "# Define the Faster R-CNN model\n",
        "def create_fasterrcnn_model(num_classes, pretrained_backbone=True):\n",
        "    import torchvision\n",
        "    backbone = torchvision.models.resnet50(weights='DEFAULT' if pretrained_backbone else None)\n",
        "    backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
        "    backbone.out_channels = 2048\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "    )\n",
        "    roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0'],\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "    model = FasterRCNN(\n",
        "        backbone=backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler,\n",
        "        min_size=800,\n",
        "        max_size=1333\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Enhanced training function with F1, accuracy, and loss tracking\n",
        "def train_model(model, data_loader, optimizer, device, num_epochs=4):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    losses_history = []\n",
        "    accuracy_history = []\n",
        "    f1_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        running_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        for i, (images, targets) in enumerate(data_loader):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            optimizer.zero_grad()\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += losses.item()\n",
        "\n",
        "            # Collect predictions and targets for metrics\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = model(images)\n",
        "            model.train()\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                # Get predictions and target labels for the current image\n",
        "                pred_labels = output['labels'].cpu().numpy()\n",
        "                true_labels = target['labels'].cpu().numpy()\n",
        "\n",
        "                # Ensure predictions and targets have the same length for this image\n",
        "                min_len = min(len(pred_labels), len(true_labels))\n",
        "                pred_labels = pred_labels[:min_len]\n",
        "                true_labels = true_labels[:min_len]\n",
        "\n",
        "                all_preds.extend(pred_labels)\n",
        "                all_targets.extend(true_labels)\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Batch {i+1}/{len(data_loader)}, Loss: {running_loss/10:.4f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "        epoch_loss = running_loss / len(data_loader)\n",
        "        losses_history.append(epoch_loss)\n",
        "\n",
        "        acc = accuracy_score(all_targets, all_preds)\n",
        "        f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
        "        accuracy_history.append(acc)\n",
        "        f1_history.append(f1)\n",
        "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f} Accuracy: {acc:.4f} F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Plotting metrics\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(range(1, num_epochs+1), losses_history, label='Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(range(1, num_epochs+1), accuracy_history, label='Accuracy', color='green')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(range(1, num_epochs+1), f1_history, label='F1 Score', color='red')\n",
        "    plt.title('Training F1 Score')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/training_metrics.png')\n",
        "    plt.close()\n",
        "    display(IPImage('/content/training_metrics.png'))\n",
        "    print(\"Training metrics saved to /content/training_metrics.png\")\n",
        "    return model\n",
        "\n",
        "# Add this validation function after the train_model function\n",
        "def validate_model(model, data_loader, device):\n",
        "    print(\"\\nStarting validation...\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Forward pass\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            total_loss += losses.item()\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                pred_labels = output['labels'].cpu().numpy()\n",
        "                true_labels = target['labels'].cpu().numpy()\n",
        "                min_len = min(len(pred_labels), len(true_labels))\n",
        "                pred_labels = pred_labels[:min_len]\n",
        "                true_labels = true_labels[:min_len]\n",
        "                all_preds.extend(pred_labels)\n",
        "                all_targets.extend(true_labels)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "    print(f\"Validation F1 Score: {f1:.4f}\\n\")\n",
        "    model.train()\n",
        "    return avg_loss, acc, f1\n",
        "\n",
        "# Function to extract frames\n",
        "def extract_frames(video_path, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return None\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_paths = []\n",
        "    frame_count = 0\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame_count += 1\n",
        "            frame_path = os.path.join(output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            frame_paths.append(frame_path)\n",
        "            print(f\"Extracting frame {frame_count}/{total_frames}\", end='\\r')\n",
        "    except Exception as e:\n",
        "        print(f\"Error during frame extraction: {str(e)}\")\n",
        "        cap.release()\n",
        "        return None\n",
        "    cap.release()\n",
        "    print(f\"\\nExtracted {frame_count} frames to {output_dir}\")\n",
        "    return frame_paths, width, height, fps\n",
        "\n",
        "# Function to visualize detections\n",
        "def visualize_detections(image, prediction, class_names, threshold=0.3):\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    image_np = (image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
        "    image_np = image_np.astype(np.uint8).copy()\n",
        "    for box, label, score in zip(prediction['boxes'].cpu().numpy(),\n",
        "                                prediction['labels'].cpu().numpy(),\n",
        "                                prediction['scores'].cpu().numpy()):\n",
        "        if score >= threshold:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            class_name = class_names.get(label, 'Unknown')\n",
        "            cv2.rectangle(image_np, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            cv2.putText(image_np, f'{class_name}: {score:.2f}', (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "    return image_np\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "from torchvision.transforms import functional as F\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "# Function to process video frames\n",
        "def process_video_frames(model, video_path, class_names, device, frames_dir, output_video_path, threshold=0.3):\n",
        "    os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return None\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_idx = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert frame to RGB and tensor\n",
        "            pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            img_tensor = F.to_tensor(pil_img).to(device)\n",
        "\n",
        "            prediction = model([img_tensor])[0]\n",
        "\n",
        "            for box, score, label in zip(prediction[\"boxes\"], prediction[\"scores\"], prediction[\"labels\"]):\n",
        "                if score >= threshold:\n",
        "                    x1, y1, x2, y2 = map(int, box)\n",
        "                    label_name = class_names[label] if label < len(class_names) else str(label)\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    cv2.putText(frame, f\"{label_name}: {score:.2f}\", (x1, y1 - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "            out_video.write(frame)\n",
        "            frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    out_video.release()\n",
        "    print(f\"Video processing complete. Output saved to {output_video_path}\")\n",
        "    return output_video_path\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    davis_root = '/content/DAVIS/DAVIS'\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # === Training section (optional) ===\n",
        "    train_dataset = DAVISDataset(davis_root, subset='train', transform=transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "    num_classes = 91\n",
        "    model = create_fasterrcnn_model(num_classes=num_classes)\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(model, train_loader, optimizer, device, num_epochs=1)\n",
        "    torch.save(model.state_dict(), '/content/fasterrcnn_resnet50_davis.pth')\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    # === Load for inference ===\n",
        "    dataset = DAVISDataset(davis_root, subset='val', transform=transform)\n",
        "    class_names = dataset.class_names\n",
        "\n",
        "    model = create_fasterrcnn_model(num_classes=num_classes)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('/content/fasterrcnn_resnet50_davis.pth', map_location=device))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Model weights not found. Please upload:\")\n",
        "        uploaded = files.upload()\n",
        "        if 'fasterrcnn_resnet50_davis.pth' in uploaded:\n",
        "            model.load_state_dict(torch.load('/content/fasterrcnn_resnet50_davis.pth', map_location=device))\n",
        "        else:\n",
        "            print(\"Model weights not uploaded. Exiting.\")\n",
        "            return\n",
        "    model.to(device)\n",
        "\n",
        "    # === Video Upload & Processing ===\n",
        "    print(\"Upload your video file (e.g., .mp4):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No video uploaded. Exiting.\")\n",
        "        return\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "\n",
        "    output_video_path = '/content/output_video.mp4'\n",
        "    result = process_video_frames(\n",
        "        model=model,\n",
        "        video_path=video_path,\n",
        "        class_names=class_names,\n",
        "        device=device,\n",
        "        frames_dir='/content/frames',\n",
        "        output_video_path=output_video_path,\n",
        "        threshold=0.3\n",
        "    )\n",
        "\n",
        "    if result:\n",
        "        print(f\"Processed video saved at: {result}\")\n",
        "        print(\"Download using this cell:\")\n",
        "        print(\"from google.colab import files; files.download('/content/output_video.mp4')\")\n",
        "    else:\n",
        "        print(\"Video processing failed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "07i8Xz_zcDcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the coco dataset based code for this project\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch torchvision scipy numpy pillow matplotlib opencv-python\n",
        "!pip install davis2017-evaluation\n",
        "!pip install --upgrade torch torchvision torchaudio\n",
        "!pip install pycocotools\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import zipfile\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from pycocotools.coco import COCO\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "from torch import nn\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.roi_heads import MultiScaleRoIAlign\n",
        "\n",
        "\n",
        "# Function to download and extract COCO dataset\n",
        "def download_and_extract_coco():\n",
        "    img_url = 'http://images.cocodataset.org/zips/train2017.zip'\n",
        "    ann_url = 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
        "\n",
        "    # Download images\n",
        "    print(\"Downloading COCO images...\")\n",
        "    img_resp = requests.get(img_url)\n",
        "    with open('train2017.zip', 'wb') as f:\n",
        "        f.write(img_resp.content)\n",
        "\n",
        "    # Unzip images\n",
        "    with zipfile.ZipFile('train2017.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/coco/images')\n",
        "\n",
        "    # Download annotations\n",
        "    print(\"Downloading COCO annotations...\")\n",
        "    ann_resp = requests.get(ann_url)\n",
        "    with open('annotations_trainval2017.zip', 'wb') as f:\n",
        "        f.write(ann_resp.content)\n",
        "\n",
        "    # Unzip annotations\n",
        "    with zipfile.ZipFile('annotations_trainval2017.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/coco/annotations')\n",
        "\n",
        "    print(\"COCO dataset downloaded and extracted!\")\n",
        "\n",
        "\n",
        "# Dataset Class\n",
        "class CocoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        self.coco = COCO(annFile)\n",
        "        self.img_ids = list(self.coco.imgs.keys())\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.img_ids[index]\n",
        "        img_info = self.coco.imgs[img_id]\n",
        "        path = img_info['file_name']\n",
        "        img = Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            boxes.append(ann['bbox'])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([img_id])\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "\n",
        "# Transformation pipeline\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    RandomHorizontalFlip(prob=0.5)\n",
        "])\n",
        "\n",
        "# Set paths for COCO dataset\n",
        "train_images_dir = '/content/coco/images/train2017'\n",
        "train_anns_file = '/content/coco/annotations/instances_train2017.json'\n",
        "val_images_dir = '/content/coco/images/val2017'\n",
        "val_anns_file = '/content/coco/annotations/instances_val2017.json'\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = CocoDataset(root=train_images_dir, annFile=train_anns_file, transform=transform)\n",
        "val_dataset = CocoDataset(root=val_images_dir, annFile=val_anns_file, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "# Model creation (Faster R-CNN with ResNet50 backbone)\n",
        "def create_fasterrcnn_model(num_classes, pretrained_backbone=True):\n",
        "    backbone = models.resnet50(weights='DEFAULT' if pretrained_backbone else None)\n",
        "    backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
        "    backbone.out_channels = 2048\n",
        "\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "    )\n",
        "\n",
        "    roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0'],\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    model = FasterRCNN(\n",
        "        backbone=backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler,\n",
        "        min_size=800,\n",
        "        max_size=1333\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, optimizer, device, num_epochs=5):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    all_train_labels = []\n",
        "    all_train_preds = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, targets in train_loader:\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += losses.item()\n",
        "\n",
        "            # Collect labels and predictions for F1 score and accuracy\n",
        "            for target, output in zip(targets, model(images)):\n",
        "                labels = target['labels'].cpu().numpy()\n",
        "                preds = output['labels'].cpu().numpy()\n",
        "                all_train_labels.extend(labels)\n",
        "                all_train_preds.extend(preds)\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
        "        f1 = f1_score(all_train_labels, all_train_preds, average='weighted')\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
        "        train_loss.append(avg_loss)\n",
        "\n",
        "    return model, train_loss, accuracy, f1\n",
        "\n",
        "\n",
        "# Validation function\n",
        "def validate_model(model, val_loader, device):\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "    all_val_labels = []\n",
        "    all_val_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            val_loss.append(losses.item())\n",
        "\n",
        "            # Calculate accuracy and F1 score for validation\n",
        "            for target, output in zip(targets, model(images)):\n",
        "                labels = target['labels'].cpu().numpy()\n",
        "                preds = output['labels'].cpu().numpy()\n",
        "                all_val_labels.extend(labels)\n",
        "                all_val_preds.extend(preds)\n",
        "\n",
        "    avg_val_loss = np.mean(val_loss)\n",
        "    accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
        "    f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
        "\n",
        "    return avg_val_loss, accuracy, f1\n",
        "\n",
        "\n",
        "# Plotting function\n",
        "def plot_metrics(train_loss, val_loss, train_accuracy, val_accuracy, train_f1, val_f1, num_epochs):\n",
        "    epochs = range(1, num_epochs + 1)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy and F1 score\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracy, label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
        "    plt.plot(epochs, train_f1, label='Training F1 Score')\n",
        "    plt.plot(epochs, val_f1, label='Validation F1 Score')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Video Processing and Inference Function\n",
        "def process_video_frames(model, video_path, output_path, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n",
        "\n",
        "    while(cap.isOpened()):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model(image_tensor)\n",
        "\n",
        "        boxes = prediction[0]['boxes']\n",
        "        labels = prediction[0]['labels']\n",
        "        scores = prediction[0]['scores']\n",
        "\n",
        "        for box, label, score in zip(boxes, labels, scores):\n",
        "            if score > threshold:\n",
        "                x1, y1, x2, y2 = box.cpu().numpy()\n",
        "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, f'ID: {label.item()} - {score:.2f}', (int(x1), int(y1 - 5)),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "\n",
        "# Main function to start training and validation\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model = create_fasterrcnn_model(num_classes=91, pretrained_backbone=True)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model, train_loss, train_accuracy, train_f1 = train_model(model, train_loader, optimizer, device, num_epochs=5)\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss, val_accuracy, val_f1 = validate_model(trained_model, val_loader, device)\n",
        "\n",
        "    # Plot the metrics\n",
        "    plot_metrics(train_loss, [val_loss]*5, [train_accuracy]*5, [val_accuracy]*5, [train_f1]*5, [val_f1]*5, 5)\n",
        "\n",
        "    # Process a video for object detection\n",
        "    process_video_frames(trained_model, 'input_video.mp4', 'output_video.mp4', device)\n",
        "    print(\"Video processing completed!\")\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(trained_model.state_dict(), 'trained_model.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ygMd9AW0hlLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is the updated versiont of original code with validation funtion, values plotting and video processing and also other Improvements.\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch torchvision scipy numpy pillow matplotlib opencv-python torchaudio\n",
        "!pip install davis2017-evaluation\n",
        "\n",
        "# # Download and extract DAVIS 2017 dataset\n",
        "!wget -O DAVIS-2017-trainval-480p.zip https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip\n",
        "!unzip -q DAVIS-2017-trainval-480p.zip -d /content/DAVIS\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import functional as F\n",
        "from IPython.display import Image as IPImage, display\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "\n",
        "# Define the DAVIS Dataset class\n",
        "class DAVISDataset(Dataset):\n",
        "    def __init__(self, davis_root, subset='val', transform=None):\n",
        "        self.davis_root = davis_root\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "        self.img_dir = os.path.join(davis_root, 'JPEGImages', '480p')\n",
        "        self.anno_dir = os.path.join(davis_root, 'Annotations', '480p')\n",
        "        with open(os.path.join(davis_root, 'ImageSets', '2017', f'{subset}.txt'), 'r') as f:\n",
        "            self.sequences = f.read().splitlines()\n",
        "        self.class_map = {\n",
        "            'bear': 1, 'bmx-bumps': 2, 'boat': 3, 'boxing-fisheye': 4, 'breakdance-flare': 5,\n",
        "            'bus': 6, 'car-turn': 7, 'cat-girl': 8, 'classic-car': 9, 'color-run': 10,\n",
        "            'crossing': 11, 'dance-jump': 12, 'dancing': 13, 'disc-jockey': 14, 'dog-agility': 15,\n",
        "            'dog-gooses': 16, 'dogs-scale': 17, 'drift-turn': 18, 'drone': 19, 'elephant': 20,\n",
        "            'flamingo': 21, 'hike': 22, 'hockey': 23, 'horsejump-low': 24, 'kid-football': 25,\n",
        "            'kite-walk': 26, 'koala': 27, 'lady-running': 28, 'lindy-hop': 29, 'longboard': 30,\n",
        "            'lucia': 31, 'mallard-fly': 32, 'mallard-water': 33, 'miami-surf': 34, 'motocross-bumps': 35,\n",
        "            'motorbike': 36, 'night-race': 37, 'paragliding': 38, 'planes-water': 39, 'rallye': 40,\n",
        "            'rhino': 41, 'rollerblade': 42, 'schoolgirls': 43, 'scooter-board': 44, 'scooter-gray': 45,\n",
        "            'sheep': 46, 'skate-park': 47, 'snowboard': 48, 'soccerball': 49, 'stroller': 50,\n",
        "            'stunt': 51, 'surf': 52, 'swing': 53, 'tennis': 54, 'tractor-sand': 55,\n",
        "            'train': 56, 'tuk-tuk': 57, 'upside-down': 58, 'varanus-cage': 59, 'walking': 60,\n",
        "            'bike-packing': 61, 'blackswan': 62, 'bmx-trees': 63, 'breakdance': 64, 'camel': 65,\n",
        "            'car-roundabout': 66, 'car-shadow': 67, 'cows': 68, 'dance-twirl': 69, 'dog': 70,\n",
        "            'dogs-jump': 71, 'drift-chicane': 72, 'drift-straight': 73, 'goat': 74, 'gold-fish': 75,\n",
        "            'horsejump-high': 76, 'india': 77, 'judo': 78, 'kite-surf': 79, 'lab-coat': 80,\n",
        "            'libby': 81, 'loading': 82, 'mbike-trick': 83, 'motocross-jump': 84, 'paragliding-launch': 85,\n",
        "            'parkour': 86, 'pigs': 87, 'scooter-black': 88, 'shooting': 89, 'soapbox': 90\n",
        "        }\n",
        "        self.class_names = {v: k for k, v in self.class_map.items()}\n",
        "        self.samples = []\n",
        "        for seq in self.sequences:\n",
        "            img_seq_dir = os.path.join(self.img_dir, seq)\n",
        "            anno_seq_dir = os.path.join(self.anno_dir, seq)\n",
        "            img_files = sorted(os.listdir(img_seq_dir))\n",
        "            for img_file in img_files:\n",
        "                if img_file.endswith('.jpg'):\n",
        "                    frame_num = img_file.split('.')[0]\n",
        "                    anno_file = f\"{frame_num}.png\"\n",
        "                    if os.path.exists(os.path.join(anno_seq_dir, anno_file)):\n",
        "                        self.samples.append((\n",
        "                            os.path.join(img_seq_dir, img_file),\n",
        "                            os.path.join(anno_seq_dir, anno_file),\n",
        "                            seq\n",
        "                        ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, seq_name = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        image_np = np.array(image)\n",
        "        mask_np = np.array(mask)\n",
        "        obj_ids = np.unique(mask_np)[1:] if np.unique(mask_np).size > 1 else []\n",
        "        if len(obj_ids) == 0:\n",
        "            masks = []\n",
        "        else:\n",
        "            masks = mask_np == obj_ids[:, None, None]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        class_label = self.class_map.get(seq_name, 1)\n",
        "        for i, mask in enumerate(masks):\n",
        "            pos = np.where(mask)\n",
        "            if len(pos[0]) > 0:\n",
        "                xmin = np.min(pos[1])\n",
        "                xmax = np.max(pos[1])\n",
        "                ymin = np.min(pos[0])\n",
        "                ymax = np.max(pos[0])\n",
        "                if xmax > xmin and ymax > ymin:\n",
        "                    boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    labels.append(class_label)\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros(0, dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': image_id,\n",
        "            'area': area,\n",
        "            'iscrowd': iscrowd\n",
        "        }\n",
        "        if self.transform:\n",
        "            image, target = self.transform(image, target)\n",
        "        return image, target\n",
        "\n",
        "# Define transformation classes\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "class Normalize:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, target\n",
        "\n",
        "class RandomHorizontalFlip:\n",
        "    def __init__(self, prob=0.5):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = F.hflip(image)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "            target[\"boxes\"] = bbox\n",
        "        return image, target\n",
        "\n",
        "# Define the Faster R-CNN model\n",
        "def create_fasterrcnn_model(num_classes, pretrained_backbone=True):\n",
        "    import torchvision\n",
        "    backbone = torchvision.models.resnet50(weights='DEFAULT' if pretrained_backbone else None)\n",
        "    backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
        "    backbone.out_channels = 2048\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "    )\n",
        "    roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0'],\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "    model = FasterRCNN(\n",
        "        backbone=backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler,\n",
        "        min_size=800,\n",
        "        max_size=1333\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    \"\"\" Compute IoU between two boxes: box1 and box2 are tensors (x1, y1, x2, y2) \"\"\"\n",
        "    xA = max(box1[0], box2[0])\n",
        "    yA = max(box1[1], box2[1])\n",
        "    xB = min(box1[2], box2[2])\n",
        "    yB = min(box1[3], box2[3])\n",
        "    inter_area = max(0, xB - xA) * max(0, yB - yA)\n",
        "    box1_area = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
        "    box2_area = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
        "    union = box1_area + box2_area - inter_area\n",
        "    return inter_area / union if union != 0 else 0\n",
        "\n",
        "# Enhanced training function with F1, accuracy, and loss tracking\n",
        "def train_model(model, data_loader, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    loss_history, acc_history, precision_history, recall_history, f1_history = [], [], [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_running_loss = 0.0\n",
        "        all_preds, all_targets = [], []\n",
        "\n",
        "        for images, targets in data_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_running_loss += total_loss.item()\n",
        "\n",
        "            # Prediction for metrics\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = model(images)\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                if len(output['boxes']) == 0 or len(target['boxes']) == 0:\n",
        "                    warnings.warn(\"Empty prediction or target, skipping this sample.\")\n",
        "                    continue\n",
        "\n",
        "                pred_boxes = output['boxes'].cpu()\n",
        "                pred_labels = output['labels'].cpu()\n",
        "                tgt_boxes = target['boxes'].cpu()\n",
        "                tgt_labels = target['labels'].cpu()\n",
        "\n",
        "                # Simple match logic using IoU > 0.5\n",
        "                matched_preds, matched_targets = [], []\n",
        "                for i, tgt_box in enumerate(tgt_boxes):\n",
        "                    best_iou, best_idx = 0, -1\n",
        "                    for j, pred_box in enumerate(pred_boxes):\n",
        "                        iou = compute_iou(tgt_box, pred_box)\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_idx = j\n",
        "                    if best_iou > 0.5:\n",
        "                        matched_preds.append(pred_labels[best_idx].item())\n",
        "                        matched_targets.append(tgt_labels[i].item())\n",
        "\n",
        "                if matched_preds:\n",
        "                    all_preds.extend(matched_preds)\n",
        "                    all_targets.extend(matched_targets)\n",
        "\n",
        "        # Compute metrics\n",
        "        epoch_loss = epoch_running_loss / len(data_loader)\n",
        "        epoch_acc = accuracy_score(all_targets, all_preds) if all_preds else 0.0\n",
        "        epoch_precision = precision_score(all_targets, all_preds, average='weighted', zero_division=0) if all_preds else 0.0\n",
        "        epoch_recall = recall_score(all_targets, all_preds, average='weighted', zero_division=0) if all_preds else 0.0\n",
        "        epoch_f1 = f1_score(all_targets, all_preds, average='weighted') if all_preds else 0.0\n",
        "\n",
        "        loss_history.append(epoch_loss)\n",
        "        acc_history.append(epoch_acc)\n",
        "        precision_history.append(epoch_precision)\n",
        "        recall_history.append(epoch_recall)\n",
        "        f1_history.append(epoch_f1)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}, F1: {epoch_f1:.4f}\")\n",
        "\n",
        "    return loss_history, acc_history, precision_history, recall_history, f1_history\n",
        "\n",
        "# Add this validation function after the train_model function\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    all_preds, all_targets = []\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Compute loss (assuming model returns losses)\n",
        "            loss_dict = model(images, targets)\n",
        "            # Sum up all losses (e.g., classification and bbox regression losses)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            val_loss += losses.item()\n",
        "\n",
        "            # Handle predictions and targets for evaluation\n",
        "            for output, target in zip(outputs, targets):\n",
        "                if len(output['boxes']) == 0 or len(target['boxes']) == 0:\n",
        "                    warnings.warn(\"Empty prediction or target in validation.\")\n",
        "                    continue\n",
        "\n",
        "                pred_boxes = output['boxes'].cpu()\n",
        "                pred_labels = output['labels'].cpu()\n",
        "                tgt_boxes = target['boxes'].cpu()\n",
        "                tgt_labels = target['labels'].cpu()\n",
        "\n",
        "                matched_preds, matched_targets = [], []\n",
        "                for i, tgt_box in enumerate(tgt_boxes):\n",
        "                    best_iou, best_idx = 0, -1\n",
        "                    for j, pred_box in enumerate(pred_boxes):\n",
        "                        iou = compute_iou(tgt_box, pred_box)\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_idx = j\n",
        "                    if best_iou > 0.5:\n",
        "                        matched_preds.append(pred_labels[best_idx].item())\n",
        "                        matched_targets.append(tgt_labels[i].item())\n",
        "\n",
        "                if matched_preds:\n",
        "                    all_preds.extend(matched_preds)\n",
        "                    all_targets.extend(matched_targets)\n",
        "\n",
        "    # Compute metrics: Accuracy, Precision, Recall, F1 Score\n",
        "    epoch_acc = accuracy_score(all_targets, all_preds) if all_preds else 0.0\n",
        "    epoch_precision = precision_score(all_targets, all_preds, average='weighted', zero_division=0) if all_preds else 0.0\n",
        "    epoch_recall = recall_score(all_targets, all_preds, average='weighted', zero_division=0) if all_preds else 0.0\n",
        "    epoch_f1 = f1_score(all_targets, all_preds, average='weighted') if all_preds else 0.0\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {epoch_acc:.4f}, Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}, F1: {epoch_f1:.4f}\")\n",
        "\n",
        "    # Return loss and metrics\n",
        "    return val_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1\n",
        "\n",
        "\n",
        "def plot_training_curves(loss_history, acc_history, precision_history, recall_history, f1_history):\n",
        "    epochs = range(1, len(loss_history)+1)\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(epochs, loss_history, 'r-', label='Loss')\n",
        "    plt.plot(epochs, acc_history, 'b--', label='Accuracy')\n",
        "    plt.plot(epochs, precision_history, 'g-.', label='Precision')\n",
        "    plt.plot(epochs, recall_history, 'm-', label='Recall')\n",
        "    plt.plot(epochs, f1_history, 'c-', label='F1 Score')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Metric\")\n",
        "    plt.title(\"Training Metrics Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_validation_metrics(val_losses, val_accuracies, val_precision, val_recall, val_f1_scores):\n",
        "    epochs = list(range(1, len(val_losses)+1))\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, val_losses, 'r-o')\n",
        "    plt.title(\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, val_accuracies, 'g-o')\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, val_f1_scores, 'b-o')\n",
        "    plt.title(\"Validation F1 Score\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Function to extract frames\n",
        "def extract_frames(video_path, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return None\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_paths = []\n",
        "    frame_count = 0\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame_count += 1\n",
        "            frame_path = os.path.join(output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            frame_paths.append(frame_path)\n",
        "            print(f\"Extracting frame {frame_count}/{total_frames}\", end='\\r')\n",
        "    except Exception as e:\n",
        "        print(f\"Error during frame extraction: {str(e)}\")\n",
        "        cap.release()\n",
        "        return None\n",
        "    cap.release()\n",
        "    print(f\"\\nExtracted {frame_count} frames to {output_dir}\")\n",
        "    return frame_paths, width, height, fps\n",
        "\n",
        "# Function to visualize detections\n",
        "def visualize_detections(image, prediction, class_names, threshold=0.3):\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    image_np = (image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
        "    image_np = image_np.astype(np.uint8).copy()\n",
        "    for box, label, score in zip(prediction['boxes'].cpu().numpy(),\n",
        "                                prediction['labels'].cpu().numpy(),\n",
        "                                prediction['scores'].cpu().numpy()):\n",
        "        if score >= threshold:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            class_name = class_names.get(label, 'Unknown')\n",
        "            cv2.rectangle(image_np, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            cv2.putText(image_np, f'{class_name}: {score:.2f}', (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "    return image_np\n",
        "\n",
        "# Function to process video frames\n",
        "def process_video_frames(model, video_path, class_names, device, frames_dir='/content/frames', output_frames_dir='/content/output_frames', threshold=0.3):\n",
        "    if os.path.exists(frames_dir):\n",
        "        shutil.rmtree(frames_dir)\n",
        "    if os.path.exists(output_frames_dir):\n",
        "        shutil.rmtree(output_frames_dir)\n",
        "    os.makedirs(frames_dir)\n",
        "    os.makedirs(output_frames_dir)\n",
        "    frame_paths, width, height, fps = extract_frames(video_path, frames_dir)\n",
        "    if frame_paths is None:\n",
        "        print(\"Error: Frame extraction failed.\")\n",
        "        return None\n",
        "    total_frames = len(frame_paths)\n",
        "    if total_frames == 0:\n",
        "        print(\"Error: No frames extracted from video.\")\n",
        "        return None\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    model.eval()\n",
        "    sample_frames = []\n",
        "    frame_count = 0\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for frame_path in frame_paths:\n",
        "                frame_count += 1\n",
        "                print(f\"Processing frame {frame_count}/{total_frames}\", end='\\r')\n",
        "                frame_rgb = cv2.cvtColor(cv2.imread(frame_path), cv2.COLOR_BGR2RGB)\n",
        "                image = Image.fromarray(frame_rgb)\n",
        "                image_tensor, _ = transform(image, {})\n",
        "                image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "                outputs = model(image_tensor)[0]\n",
        "                annotated_frame = visualize_detections(image_tensor[0], outputs, class_names, threshold)\n",
        "                output_frame_path = os.path.join(output_frames_dir, f\"annotated_frame_{frame_count:06d}.jpg\")\n",
        "                cv2.imwrite(output_frame_path, cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
        "                if frame_count % (total_frames // 5 + 1) == 0 and len(sample_frames) < 5:\n",
        "                    sample_frames.append(annotated_frame)\n",
        "                del image_tensor, outputs\n",
        "                torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during frame processing: {str(e)}\")\n",
        "        return None\n",
        "    print(f\"\\nProcessed {frame_count} frames. Annotated frames saved to {output_frames_dir}\")\n",
        "    for i, frame in enumerate(sample_frames):\n",
        "        sample_path = f'/content/sample_frame_{i}.png'\n",
        "        cv2.imwrite(sample_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "        display(IPImage(sample_path))\n",
        "    shutil.rmtree(frames_dir)\n",
        "    print(\"Cleaned up temporary frames directory\")\n",
        "    return output_frames_dir\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    davis_root = '/content/DAVIS/DAVIS'\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        RandomHorizontalFlip(prob=0.5)\n",
        "    ])\n",
        "\n",
        "    # Optional: Train the model (uncomment to train)\n",
        "    train_dataset = DAVISDataset(davis_root, subset='train', transform=transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "    num_classes = 91\n",
        "    model = create_fasterrcnn_model(num_classes=num_classes)\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "    model = train_model(model, train_loader, optimizer, device, num_epochs=5)\n",
        "    torch.save(model.state_dict(), '/content/fasterrcnn_resnet50_davis.pth')\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    from dataset import get_davis_dataloader  # Custom dataloader\n",
        "    from model import get_model  # Your Faster R-CNN model setup\n",
        "\n",
        "    # Load model and data\n",
        "    model = get_model(num_classes=91)  # Adjust num_classes\n",
        "    model.to(device)\n",
        "\n",
        "    _, val_loader = get_davis_dataloader(batch_size=2)  # Custom function\n",
        "\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    num_epochs = 3\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        val_loss, val_acc, val_f1 = validate_model(model, val_loader, device)\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        val_f1_scores.append(val_f1)\n",
        "\n",
        "    # Plot all metrics after validation\n",
        "    plot_validation_metrics(val_losses, val_accuracies, val_f1_scores)\n",
        "\n",
        "    # Load dataset for class names\n",
        "    dataset = DAVISDataset(davis_root, subset='val', transform=transform)\n",
        "    class_names = dataset.class_names\n",
        "\n",
        "    # Load the model\n",
        "    num_classes = 91\n",
        "    model = create_fasterrcnn_model(num_classes=num_classes)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('/content/fasterrcnn_resnet50_davis.pth', map_location=device))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Model weights file 'fasterrcnn_resnet50_davis.pth' not found.\")\n",
        "        print(\"Please upload the trained model or uncomment the training section to train the model.\")\n",
        "        print(\"To proceed without training, upload 'fasterrcnn_resnet50_davis.pth' now:\")\n",
        "        uploaded = files.upload()\n",
        "        if 'fasterrcnn_resnet50_davis.pth' in uploaded:\n",
        "            model.load_state_dict(torch.load('/content/fasterrcnn_resnet50_davis.pth', map_location=device))\n",
        "        else:\n",
        "            print(\"Error: Model weights not uploaded. Cannot proceed.\")\n",
        "            return\n",
        "    model.to(device)\n",
        "\n",
        "    # Upload video\n",
        "    print(\"Please upload your video file (e.g., MP4 format):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"Error: No video file uploaded.\")\n",
        "        return\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # Process video frames\n",
        "    output_frames_dir = process_video_frames(\n",
        "        model, video_path, class_names, device,\n",
        "        frames_dir='/content/frames',\n",
        "        output_frames_dir='/content/output_frames',\n",
        "        threshold=0.3\n",
        "    )\n",
        "\n",
        "    if output_frames_dir:\n",
        "        print(f\"Processed frames saved to {output_frames_dir}\")\n",
        "        print(\"To download all frames, run the following cell to zip the output_frames directory:\")\n",
        "        print(\"Then download the zip file from the Colab file explorer.\")\n",
        "        # Zip the output frames for easy download\n",
        "        !zip -r /content/output_frames.zip /content/output_frames\n",
        "    else:\n",
        "        print(\"Frame processing failed. Please check error messages.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "3UlEheAkwAnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}